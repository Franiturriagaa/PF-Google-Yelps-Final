{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from google.cloud import bigquery\n",
    "from pandas_gbq import read_gbq\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import joblib\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carga de datos desde la nube\n",
    "project_id = 'mystic-cable-435413-b4'\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "FROM `mystic-cable-435413-b4.googleyelp.yelp-business`\n",
    "\"\"\"\n",
    "\n",
    "#Cargar los datos desde BigQuery usando pandas-gbq\n",
    "df = pd.read_gbq(query, project_id=project_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Análisis de Sentimiento (suponiendo que ya tienes las columnas de sentimiento)\n",
    "df['avg_sentiment'] = df[['reviews_sentiment', 'tips_sentiment']].mean(axis=1).fillna(0)\n",
    "\n",
    "# 3. Agrupación por cluster en latitud y longitud\n",
    "X_geo = df[['latitude', 'longitude']]\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(X_geo)\n",
    "\n",
    "# 4. Combinar puntaje por stars y analisis de sentimiento (multiplicado por 7.5 para igualar importancia con stars)\n",
    "df['combined_score'] = df['stars'] + df['avg_sentiment'] * 7.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el valor mínimo y máximo del combined_score\n",
    "min_combined_score = df['combined_score'].min()\n",
    "max_combined_score = df['combined_score'].max()\n",
    "\n",
    "# Normalizar el combined_score entre 1 y 10\n",
    "df['combined_score'] = 1 + (df['combined_score'] - min_combined_score) * (10 - 1) / (max_combined_score - min_combined_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluacion de importancia:\n",
      "               importancia\n",
      "stars             0.536435\n",
      "avg_sentiment     0.462141\n",
      "postal_code       0.000678\n",
      "longitude         0.000378\n",
      "latitude          0.000368\n"
     ]
    }
   ],
   "source": [
    "# Agrupar negocios por 'main category', 'categories' y 'cluster', cálculo de puntaje promedio\n",
    "df_grouped = df.groupby(['main category', 'categories', 'cluster']).agg({\n",
    "    'combined_score': 'mean',   \n",
    "    'stars': 'mean',            \n",
    "    'avg_sentiment': 'mean',    \n",
    "    'business_id': 'count',     \n",
    "    'latitude': 'mean',         \n",
    "    'longitude': 'mean',        \n",
    "    'postal_code': 'first',\n",
    "    \n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "# 5. Machine Learning Training/Testing\n",
    "# Variables usadas:\n",
    "X = df[['latitude', 'longitude', 'stars', 'avg_sentiment', 'postal_code']]\n",
    "y = df['combined_score']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalado de datos\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 9. Modelo XGBoost para ajuste de hiperparametros\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "model = XGBRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 10. Evaluacion del modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 11. Contribucion de variables\n",
    "feature_importances = pd.DataFrame(best_model.feature_importances_, index=X.columns, columns=['importancia']).sort_values('importancia', ascending=False)\n",
    "print(\"Evaluacion de importancia:\")\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo de XGBoost\n",
    "best_model.save_model(\"xgboost_model.json\")\n",
    "\n",
    "# Guardar el scaler con pickle\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Categorias - Puntaje (Predicción del modelo):\n",
      "1. Beauty & Spa -  7.17\n",
      "2. Active life -  6.96\n",
      "3. Pets -  6.88\n",
      "4. Cofee Shop -  6.86\n",
      "5. Shopping -  6.77\n"
     ]
    }
   ],
   "source": [
    "# 13. Encontrar las 5 top categorías predichas por el modelo entrenado\n",
    "\n",
    "# Utilizar las variables de entrada que has definido antes como 'X'\n",
    "X_for_prediction = df_grouped[['latitude', 'longitude', 'stars', 'avg_sentiment', 'postal_code']]\n",
    "\n",
    "# Escalar los datos como en el entrenamiento\n",
    "X_for_prediction_scaled = scaler.transform(X_for_prediction)\n",
    "\n",
    "# Predecir el combined_score usando el modelo entrenado\n",
    "df_grouped['predicted_combined_score'] = best_model.predict(X_for_prediction_scaled)\n",
    "\n",
    "# Seleccionar las 5 categorías con el mejor combined_score predicho\n",
    "top_5_categories = df_grouped.groupby('main category').agg({\n",
    "    'predicted_combined_score': 'mean'\n",
    "}).reset_index().nlargest(5, 'predicted_combined_score')\n",
    "\n",
    "# Resetear el índice para una mejor visualización\n",
    "top_5_categories = top_5_categories.reset_index(drop=True)\n",
    "\n",
    "# Imprimir las 5 top categorías en orden de puntaje\n",
    "print(\"Top 5 Categorias - Puntaje (Predicción del modelo):\")\n",
    "for i, row in top_5_categories.iterrows():\n",
    "    print(f\"{i+1}. {row['main category']} -  {row['predicted_combined_score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 2 Subcategorías para 'Shopping':\n",
      "1. Tennis - 9.03\n",
      "2. Spray Tanning - 8.69\n"
     ]
    }
   ],
   "source": [
    "# 14. Seleccionar una de las top 5 categorías\n",
    "selected_main_category = input(\"Por favor, selecciona una de las top 5 main categories: \")\n",
    "\n",
    "# 15. Encontrar las mejores 2 subcategorías dentro de la main category seleccionada\n",
    "df_filtered = df_grouped[df_grouped['main category'] == selected_main_category]\n",
    "\n",
    "# Agrupar por 'categories' dentro de la 'main category' seleccionada utilizando el combined score predicho\n",
    "top_2_subcategories = df_filtered.groupby('categories').agg({\n",
    "    'predicted_combined_score': 'mean'  # Usar el combined_score predicho por el modelo\n",
    "}).reset_index().nlargest(2, 'predicted_combined_score')\n",
    "\n",
    "# Imprimir las 2 mejores subcategorías\n",
    "print(f\"Top 2 Subcategorías para '{selected_main_category}':\")\n",
    "for i, row in enumerate(top_2_subcategories.itertuples(), 1):\n",
    "    print(f\"{i}. {row.categories} - {row.predicted_combined_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Posibles Ubicaciones para la subcategoría 'Spray Tanning':\n",
      "   postal_code\n",
      "1        19107\n",
      "2        19122\n"
     ]
    }
   ],
   "source": [
    "# Pedir al usuario que seleccione una subcategoría\n",
    "selected_subcategory_index = int(input(\"\\nPor favor selecciona una subcategoría (1 o 2): \")) - 1\n",
    "selected_subcategory = top_2_subcategories.iloc[selected_subcategory_index]['categories']\n",
    "\n",
    "# 18. Distancia Haversine para evitar que las ubicaciones sean iguales\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Radio de la Tierra en kilómetros\n",
    "    d_lat = np.radians(lat2 - lat1)\n",
    "    d_lon = np.radians(lon2 - lon1)\n",
    "    a = np.sin(d_lat / 2) ** 2 + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(d_lon / 2) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c * 1000  # Convertir a metros\n",
    "\n",
    "# 19. Predecir la primera ubicación basada en la subcategoría seleccionada usando el modelo entrenado\n",
    "def recommend_top_1_location_for_subcategory(subcategory):\n",
    "    subcategory_data = df_grouped[df_grouped['categories'] == subcategory]\n",
    "    \n",
    "    # Seleccionar las características necesarias para la predicción\n",
    "    X_subcategory = subcategory_data[['latitude', 'longitude', 'stars', 'avg_sentiment', 'postal_code']]\n",
    "    \n",
    "    # Escalar las características de la misma manera que durante el entrenamiento\n",
    "    X_subcategory_scaled = scaler.transform(X_subcategory)\n",
    "    \n",
    "    # Predecir el combined_score usando el modelo entrenado\n",
    "    subcategory_data['predicted_combined_score'] = best_model.predict(X_subcategory_scaled)\n",
    "    \n",
    "    # Seleccionar la mejor ubicación basada en la predicción del modelo\n",
    "    top_1_location = subcategory_data.nlargest(1, 'predicted_combined_score')\n",
    "    return top_1_location\n",
    "\n",
    "# 20. Predecir la segunda ubicación basada en análisis de subcategorías similares usando el modelo entrenado y Haversine\n",
    "def recommend_top_1_location_from_similar_categories(subcategory, top_1_lat, top_1_lon):\n",
    "    # Filtrar categorías con análisis de sentimiento similar\n",
    "    similar_categories_data = df_grouped[\n",
    "        (df_grouped['categories'] != subcategory) & \n",
    "        (abs(df_grouped['avg_sentiment'] - df_grouped[df_grouped['categories'] == subcategory]['avg_sentiment'].mean()) < 0.1)\n",
    "    ]\n",
    "    \n",
    "    # Si se encuentran categorías similares\n",
    "    if len(similar_categories_data) > 0:        # Seleccionar las características necesarias para la predicción\n",
    "        X_similar_categories = similar_categories_data[['latitude', 'longitude', 'stars', 'avg_sentiment', 'postal_code']]\n",
    "        \n",
    "        # Escalar las características\n",
    "        X_similar_categories_scaled = scaler.transform(X_similar_categories)\n",
    "        \n",
    "        # Predecir el combined_score usando el modelo entrenado\n",
    "        similar_categories_data['predicted_combined_score'] = best_model.predict(X_similar_categories_scaled)\n",
    "        \n",
    "        # Calcular la distancia desde la primera ubicación\n",
    "        similar_categories_data['distance'] = similar_categories_data.apply(\n",
    "            lambda row: haversine_distance(top_1_lat, top_1_lon, row['latitude'], row['longitude']), axis=1\n",
    "        )\n",
    "        \n",
    "        # Seleccionar la mejor ubicación de categorías similares que esté a más de 1000 metros (1 km)\n",
    "        top_1_other_location = similar_categories_data[similar_categories_data['distance'] > 1000].nlargest(1, 'predicted_combined_score')\n",
    "        \n",
    "        return top_1_other_location\n",
    "    else:\n",
    "        # Si no se encuentran categorías similares, seleccionamos por puntaje más cercano\n",
    "        return df_grouped[df_grouped['categories'] != subcategory].nlargest(1, 'predicted_combined_score')\n",
    "\n",
    "# 21. Obtener la primera ubicación basada en la subcategoría seleccionada\n",
    "top_1_location_subcategory = recommend_top_1_location_for_subcategory(selected_subcategory)\n",
    "top_1_location_subcategory = top_1_location_subcategory.reset_index(drop=True)\n",
    "\n",
    "# Extraer latitud y longitud de la primera ubicación\n",
    "top_1_lat = top_1_location_subcategory['latitude'].iloc[0]\n",
    "top_1_lon = top_1_location_subcategory['longitude'].iloc[0]\n",
    "\n",
    "# 22. Obtener la segunda ubicación basada en categorías similares y la distancia Haversine\n",
    "top_1_location_similar_category = recommend_top_1_location_from_similar_categories(selected_subcategory, top_1_lat, top_1_lon)\n",
    "top_1_location_similar_category = top_1_location_similar_category.reset_index(drop=True)\n",
    "\n",
    "# 23. Mostrar ambas ubicaciones con índices comenzando en 1\n",
    "top_2_locations = pd.concat([top_1_location_subcategory, top_1_location_similar_category], ignore_index=True)\n",
    "\n",
    "# Asignar índices comenzando en 1\n",
    "top_2_locations.index = top_2_locations.index + 1\n",
    "\n",
    "# Mostrar las ubicaciones con índices 1 y 2\n",
    "print(f\"\\nPosibles Ubicaciones para la subcategoría '{selected_subcategory}':\")\n",
    "print(top_2_locations[['postal_code']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Cuadratico Medio (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________\n",
    "\n",
    "## ANALISIS DE METRICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0016384779306283593\n"
     ]
    }
   ],
   "source": [
    "# Calculo Error Cuadratico Medio en Test (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBS: Un bajo valor de MSE implica que las predicciones son cercanas al valor real, buena precision y los datos en la muestra son consistentes puesto que este parametro es muy suceptible a outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-squared: 0.9998715804529332\n",
      "Test R-squared: 0.9988899789692228\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "y_train_pred = best_model.predict(X_train_scaled)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "print(f\"Train R-squared: {train_r2}\")\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Test R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBS: Se pensaria que este modelo en particular esta sobre entrenado ya que el Train R^2 es cercano a 1, sin embargo el Test R^2 es igualmente alto cercano a 1. Al ambos ser similares y proximos a 1, indica que el modelo esta generalizando bien con los datos recibidos desde Google Cloud Platform (GCP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluacion por: Validacion Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntaje Cross-validation R²: [0.99651997 0.99652959 0.99721681]\n",
      "Media de la Cross-validation R²: 0.9967554578151828\n",
      "Desviación estándar de la Cross-validation R²: 0.00032624815404414003\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Parametros para la validacion\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=50,     # Numero de arboles\n",
    "    max_depth=3,         # profundidad de arboles\n",
    "    learning_rate=0.1,   # Tasa de aprendizaje moderado\n",
    "    random_state=42,     # Aleatoriedad\n",
    "    n_jobs=-1            # Usar todos los nucleos para el analisis\n",
    ")\n",
    "\n",
    "# Desempeno de la validacion K-fold= 3 (cv=3 para optimizar el uso de memoria y tiempo de analisis)\n",
    "cv_scores = cross_val_score(xgb_model, X_train_scaled, y_train, cv=3, scoring='r2')\n",
    "\n",
    "# Cálculo de la desviación estándar\n",
    "std_dev = np.std(cv_scores)\n",
    "\n",
    "print(f\"Puntaje Cross-validation R²: {cv_scores}\")\n",
    "print(f\"Media de la Cross-validation R²: {np.mean(cv_scores)}\")\n",
    "print(f\"Desviación estándar de la Cross-validation R²: {std_dev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBS: Metodo usado para evaluar la variabilidad y la confiabilidad de modelos de ML. Entre mas altos y cercanos sean los valores de validacion, mejor el desempeño. Ya que los 3 valores de la validacion son consistentes y similares a lo largo de la muestra, indica que es capaz de generalizar sin estar sobre entrenado y con una baja desviacion estandar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
